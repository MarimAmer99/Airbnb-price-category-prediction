# -*- coding: utf-8 -*-
"""DM_Project_44.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q5cnmHlw2FWvpw8Esjmm7Ukucg9xhHXI

# Airbnb price category prediction problem

## Input (two modalities):

- summary (text data)
- image (image data)

## Output (two predictions): To predict the listing price based on the listing characteristics
- Price
- Type
 

## Data mining function

#### Classification and prediction because we need to predict the listing price based on the listing characteristics. 

## The challenges 

#### Cleaning data and removing all tags, special characters, ASCII, and white spaces from the text, remove all nan values and convert all images into the same size.

## Impact

#### We will know the range of the price.

## Steps

#### 1- Reading data and display data.
#### 2- Cleaning data and preprocessing data.
*   Remove all tags, special characters, ASCII, and white spaces from the text.
*   Convert all images into the same size.
*   Remove all nan values.


#### 4- Trials


## Protocol 
#### I intend to use hold out method.


## Search space
#### I used random's range in the code according to try and error.

## Determine good/bad hyper-parameters
#### According to the accuracy graph if there is overfitting or underfitting and then I decided to change hyperparameters of add layers.

##**The best solution**

**Trial_13**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) with different hyperprameters values and layers
* Dropout
* Regularization l2

Score:  0.66168

Private: 0.66902
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Data Preparations"""

# First let's download and unzip the dataset

! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4.zip
! unzip -q a4.zip

import os

import pandas as pd
from tqdm.notebook import tqdm
from PIL import Image
import pandas as pd
import os
import numpy as np
from ast import literal_eval
from keras.models import Sequential
from tensorflow.keras.optimizers import SGD
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional

from __future__ import absolute_import, division, print_function, unicode_literals
import collections
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from matplotlib import pyplot as plt
from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D
from tensorflow.keras.optimizers import Adam

import re
import pickle
import sklearn
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from keras import regularizers

# we find that (through the file browser on the left) there is a csv file and a 
df_train = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/train_xy.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/test_x.csv')

#Dispaly traing data
df_train

"""Let's encode the prediction labels and calculate the total number 
of unique labels. After, lets split the dataset into training set and testing set.
"""

from sklearn.model_selection import train_test_split

# labels:
df_train['type'] = df_train['type'].astype('category').cat.codes
df_train['price'] = df_train['price'].astype('category').cat.codes
len_type = len(df_train['type'].unique())
len_price = len(df_train['price'].unique())

"""#Cleaning and Preprocessing"""

nltk.download('punkt')
nltk.download('stopwords')

stemmer = SnowballStemmer("english")
stop_words = set(stopwords.words("english"))

#Embedded need to know the meaning of the sentence
def clean_text(text):
    """ steps:
        - remove any html tags (< /br> often found)
        - Keep only ASCII + European Chars and whitespace, no digits
        - remove single letter chars
        - convert all whitespaces (tabs etc.) to single wspace
        if not for embedding (but e.g. tdf-idf):
        - all lowercase
        - remove stopwords, punctuation and stemm
    """
    RE_WSPACE = re.compile(r"\s+", re.IGNORECASE)  #White Space
    RE_TAGS = re.compile(r"<[^>]+>")    #Tags
    RE_ASCII = re.compile(r"[^A-Za-zÀ-ž ]", re.IGNORECASE)    #ASCII
    RE_SINGLECHAR = re.compile(r"\b[A-Za-zÀ-ž]\b", re.IGNORECASE) #Single charachter between two spaces

    #Replace White Space, Tags, ASCII and Single charachter between two spaces with single space
    text = re.sub(RE_TAGS, " ", text)
    text = re.sub(RE_ASCII, " ", text)
    text = re.sub(RE_SINGLECHAR, " ", text)
    text = re.sub(RE_WSPACE, " ", text)
    text = str(text).lower()
    return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Traning data
# # Clean titles have less than 0 characters
# df_train["summary"] = df_train.loc[df_train["summary"].str.len() > 0, "summary"]
# df_train["summary"] = df_train["summary"].map(
#     lambda x: clean_text(x) if isinstance(x, str) else x         #(lambda) Apply the fuction in each row
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Testing data
# # Clean titles have less than 0 characters
# df_test["summary"] = df_test.loc[df_test["summary"].str.len() > 0, "summary"]
# df_test["summary"] = df_test["summary"].map(
#     lambda x: clean_text(x) if isinstance(x, str) else x         #(lambda) Apply the fuction in each row
# )

#Display training data after summary column cleaned
df_train

"""### Remove nan Values"""

#Check the number of null values in each column
df_test.isna().sum()
df_train.isna().sum()

#Remove all nan values from data
df_train.dropna()
df_test.dropna()

"""## Data Preprocessing

We have image and text data. 

- Image data: resize
- Text data: tokenization and converting to integer IDs
"""

# preprocess image data
import os
''' 
  Steps:
  1- Open file
  2- Read images as gray scall
  3- Resize images(64,64)
  4- Convert summary column to be a string: I do that to guarantee all rows are string. 
'''
def load_image(file):
    try:
        image = Image.open(
            file
        ).convert('LA').resize((64, 64))    #Read images as gray scall  #Resize images(64,64)
        arr = np.array(image)
    except:
        arr = np.zeros((64, 64, 2))
    return arr


# loading images:
x_train_image = np.array([load_image(i) for i in tqdm(df_train['image'])])

# loading summary: (force convert some of the non-string cell to string)
x_train_text = df_train['summary'].astype('str')

# get type 
y_train_type = df_train['type']

# get price
y_train_price = df_train['price']

# check image loading
import matplotlib.pyplot as plt
plt.imshow(x_train_image[0, :, :, 0])

# preprocess text data

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from pprint import pprint

vocab_size = 40000
max_len = 100


# build vocabulary from training set
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(x_train_text)


def _preprocess(list_of_text):
    return pad_sequences(
        tokenizer.texts_to_sequences(list_of_text),
        maxlen=max_len,
        padding='post',
    )
    

# padding is done inside: 
x_train_text_id = _preprocess(x_train_text)

print(x_train_text_id.shape)

# we can use the tokenizer to convert IDs to words.
pprint(tokenizer.sequences_to_texts(x_train_text_id[:5]))

print('total words in the dictionary:', tokenizer.num_words)

"""# Trials

In these trials, I apply many algorithms like CNN, GRU, and LSTM.

In all trials, I used multitasking and multimodality, and  Early Stopping to prevent overfitting.

To prevent overfitting I used in some trials Dropout and regularization L2 with Early Stopping. 

**Embedded**: is a machine learning algorithm that starts with random weights and learns an embedding for all of the words in the training dataset.

**Dropout**: is a training strategy in which randomly selected neurons are rejected. They are "dropouts" at random. This means that on the forward pass, their contribution to the activation of downstream neurons is removed temporally, and on the backward pass, any weight updates are not applied to the neuron.

**Max-pooling**: that selects the maximum element from the region of the feature map covered by the filter and the output of the max-pooling layer would be a feature map with the most prominent features from the previous feature map.

---

**Trial_1**

Text part: I used embedded

Image part: CNN(Conv, and max-pooling)

Score: 0.61141

---


**Trial_2**

Text part: I used embedded and GRU algorithm

Image part: CNN(Conv, and max-pooling)

Score: 0.52255

---



**Trial_3**

Text part: I used embedded and GRU algorithm

Image part: 
* CNN(conv, and max-pooling)
* Dropout

Score: 0.31494

---

**Trial_4**

Text part: 
* I used embedded and GRU algorithm
* Dropout

Image part: 
* CNN(conv, and max-pooling)
* Dropout

Score: 0.61141

---


**Trial_5**

Text part: 
* I used embedded and GRU algorithm with Bidirectional
* Dropout

Image part: 
* CNN(conv, and max-pooling)
* Dropout

Score: 0.60760

---

**Trial_6**

Text part: 
* I used embedded and GRU algorithm(activation function = "tanh") with Bidirectional layer

* Dropout

Image part: 
* CNN(conv, and max-pooling) with different hyperprameters values
* Dropout

Score: 0.63369

---

**Trial_7**

Text part: 
* I used embedded and GRU algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(Conv, and max-pooling) with different hyperparameters values and layers
* Dropout

Score: 0.63396
---


**Trial_8**

Text part: 
* I used embedded and GRU algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(Conv, and max-pooling) with different hyperparameters values and layers
* Dropout

Score: 0.64646

---


**Trial_9**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh")
* Dropout

Image part: 
* CNN(conv, and max-pooling) 
* Dropout

Score: 0.57499

---

**Trial_10**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) 
* Dropout
* Regularization l2

Score: 0.59076

---

**Trial_11**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) 
* Dropout
* Regularization l2

Score: 0.65353

---

**Trial_12**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) with different hyperprameters values and layers
* Dropout
* Regularization l2

Score: 0.60788

---

**Trial_13**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) with different hyperprameters values and layers
* Dropout
* Regularization l2

Score:  0.66168

##CNN
**(CNN/ConvNet): Convolutional neural network**
Convolution is the application of a filter to input to create a feature map that extracts features from the input. And then apply non-linearity and then do pooling or downsample.

**Convolution**: apply the filter in the input to create a feature map.

**Non-linearity**: we can use tanh or relu or softmax.

**Pooling**: sum or max over non-overlapping / overlapping regions.

The advantage: It detects the essential features from the input so I will use it with an image to extract the feature from it.

##Trial_1
CNN

In the first trial, I will use embedded with text data and one convolutional and max-pooling layer to extract features from images so I think the accuracy will be high.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
averaged = tf.reduce_mean(embedded, axis=1)


# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)  #2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((16, 16))(cov)   #Output: (None, 3, 3, 32)
flattened = Flatten()(pl) #flattening layer Output Shape (None, 288)


# fusion - combinig both
fused = tf.concat([averaged, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission.csv', index=False)



"""## Score: 0.61141

##Trial_2
GRU Algorithm

According to the previous trial, accuracy decreased so I will try the GRU algorithm as  GRUs are improved versions of the standard recurrent neural networks and it is solved the vanishing gradient problem of a standard RNN.

So I thought the accuracy will be increased as the GRU has a memory.

Hyperprameters:

1. **units** – Dimensionality of the output space.
2. **activation** – Activation function to use. Default: hyperbolic tangent (tanh).
3. **recurrent_activation** – Activation function to use for the recurrent step. 
4. **kernel_initializer** – Initializer for the kernel weights matrix, used for the linear transformation of the inputs. Default: glorot_uniform.

I will try the different value of these hyperparameters in the following trails.

In this trial i will use units only

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
gru = GRU(150)(embedded)


# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)  #2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((16, 16))(cov) #Output: (None, 3, 3, 32)
flattened = Flatten()(pl)   #flattening layer Output Shape (None, 288)


# fusion - combinig both
fused = tf.concat([gru, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_2.csv', index=False)

"""## Score: 0.52255

##Trial_3
GRU Algorithm

Change of hyperprameters value

I thought the accuracy will be increased.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
gru = GRU(100, activation='tanh')(embedded)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)    #2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
drop_out = Dropout(0.2)(cov)            #Output: (None, 49, 49, 32)
pl = MaxPool2D((16, 16))(drop_out)      #Output:  (None, 3, 3, 32)
flattened = Flatten()(pl)               #flattening layer Output Shape (None, 288)


# fusion - combinig both
fused = tf.concat([gru, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_3.csv', index=False)

"""## Score: Score: 0.31494

##Trial_4
GRU Algorithm

There is overfitting so I will try to add dropout in CNN(image).

Image: dropout.

I thought the overfitting will be decreased because dropout prevent the overfitting.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
gru = GRU(100, activation='tanh')(embedded)
gru = Dropout(0.2)(gru)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)       # 2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
drop_out = Dropout(0.2)(cov)               # output: (None, 49, 49, 32)
pl = MaxPool2D((16, 16))(drop_out)         # output: (None, 3, 3, 32)
flattened = Flatten()(pl)                  # flattening layer Output Shape (None, 288)



# fusion - combinig both
fused = tf.concat([gru, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_4.csv', index=False)

"""## Score: 0.61141

##Trial_5
GRU Algorithm by Bidirectional

Image: dropout

I thought the accuracy will be increased as the GRU has a memory so it will reach the correct mean.

### Building a Learning model
"""

keras.backend.clear_session()
# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(GRU(100, activation='tanh'))(embedded)
bidirect = Dropout(0.2)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)                  # 2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((16, 16))(cov)                         # Output: (None, 3, 3, 32)
drop_out = Dropout(0.2,name='dropout_conv1')(pl)      # Output: (None, 3, 3, 32)
cov2 = Conv2D(32, (3, 3))(drop_out)                   # Output: (None, 1, 1, 32)
drop_out = Dropout(0.2,name='dropout_conv2')(cov2)    # Output: (None, 1, 1, 32)
flattened = Flatten()(drop_out)                       # flattening layer Output Shape (None, 32)

# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_5.csv', index=False)

"""There is also overfitting, but it is less than the previous attempt.

## Score: 0.60760

##Trial_6
GRU Algorithm by using Bidirectional

Image: dropout

I will use activation function "tanh" and Bidirectional algorithm

**Bidirectional Algorithm**

To enable straight (past) and reverse traversal of input (future), Bidirectional RNNs, or BRNNs, are used. A BRNN is a combination of two RNNs - one RNN moves forward, beginning from the start of the data sequence, and the other, moves backward, beginning from the end of the data sequence.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(GRU(100, activation='tanh'))(embedded)
bidirect = Dropout(0.3)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16))(in_image)                  # 2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((7, 7))(cov)                           #Output: (None, 7, 7, 32)
drop_out = Dropout(0.2,name='dropout_conv1')(pl)      #Output: (None, 7, 7, 32)
cov2 = Conv2D(32, (5, 5))(drop_out)                   #Output: (None, 3, 3, 32)
drop_out = Dropout(0.2,name='dropout_conv2')(cov2)    #Output: (None, 3, 3, 32)
flattened = Flatten()(drop_out)                       # flattening layer Output Shape (None, 288)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_6.csv', index=False)

"""## Score: 0.63369

##Trial_7
GRU Algorithm by using Bidirectional

Image: dropout

I will use activation function "tanh" and Bidirectional algorithm

**Bidirectional Algorithm**

To enable straight (past) and reverse traversal of input (future), Bidirectional RNNs, or BRNNs, are used. A BRNN is a combination of two RNNs - one RNN moves forward, beginning from the start of the data sequence, and the other, moves backward, beginning from the end of the data sequence.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(GRU(100, activation='tanh'))(embedded)
bidirect = Dropout(0.2)(bidirect)


# image part 
# simple conv2d. you can change it to anything else as needed

cov = Conv2D(32, (16, 16), name= "conv1")(in_image)       # 2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((8, 8))(cov)                               # Output: (None, 6, 6, 32)
drop_out1 = Dropout(0.2,name='dropout_conv1')(pl)         # Output: (None, 6, 6, 32)
cov2 = Conv2D(32, (6, 6), name= "conv2")(drop_out1)       # Output: (None, 1, 1, 32)
drop_out2 = Dropout(0.2,name='dropout_conv2')(cov2)       # Output: (None, 1, 1, 32)
flattened = Flatten()(drop_out2)                          # flattening layer Output Shape (None, 32)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_7.csv', index=False)

"""##Score: 0.63396

##Trial_8
GRU Algorithm by using Bidirectional

Image: dropout

I need to improve the model so I will add some convolutional layers and max-pooling layer and dropout to prevent overfitting in the model.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(GRU(100, activation='tanh'))(embedded)
bidirect = Dropout(0.2)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (16, 16), name= "conv1")(in_image)         # 2D Convolutional layer with 32 filters and filter size of 16x16 Output: (None, 49, 49, 32)
pl = MaxPool2D((8, 8))(cov)                                 # Output: (None, 6, 6, 32)
drop_out1 = Dropout(0.3,name='dropout_conv1')(pl)           # Output: (None, 6, 6, 32)
cov2 = Conv2D(32, (6, 6), name= "conv2")(drop_out1)         # Output: (None, 1, 1, 32)
drop_out2 = Dropout(0.3,name='dropout_conv2')(cov2)         # Output: (None, 1, 1, 32)
flattened = Flatten()(drop_out2)                            # flattening layer Output Shape (None, 32)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_8.csv', index=False)

"""## Score: 0.64646

The accuracy is good so I will try the LSTM algorithm also to improve the accuracy.

##Trial_9
LSTM Algorithm without Bidirectional

I will use in LSTM activation function "tanh" and in the image, I will use dropout to prevent overfitting.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
lstm_model = LSTM(100, activation='tanh')(embedded)
lstm_model = Dropout(0.3)(lstm_model)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (15, 15), name = "cov1")(in_image)     # 2D Convolutional layer with 32 filters and filter size of 15*15 Output: (None, 50, 50, 32)
pl = MaxPool2D((10, 10), name = "maxp_cov1")(cov)       # Output: (None, 5, 5, 32)
drop_out1 = Dropout(0.4, name = "dropout_cov1")(pl)     # Output: (None, 5, 5, 32)
flattened = Flatten()(drop_out1)                        # flattening layer Output Shape (None, 800)


# fusion - combinig both
fused = tf.concat([lstm_model, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_9.csv', index=False)

"""## Score: 0.62934

##Trial_10
LSTM Algorithm by using Bidirectional.

To increase the accuracy I will add a Bidirectional layer and add some convolutional layer and max-pooling.

To prevent overfitting I used regularization l2 in the convolutional layer and dropout layer.

so I think the accuracy will be increased.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(LSTM(100, activation='tanh'))(embedded)
bidirect = Dropout(0.3)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (15, 15), name = "cov1", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(in_image)     # 2D Convolutional layer with 32 filters and filter size of 15*15 and regularizer L2. Output: (None, 50, 50, 32)
pl = MaxPool2D((10, 10), name = "maxp_cov1")(cov)                                                      # Output: (None, 5, 5, 32)
drop_out1 = Dropout(0.3, name = "dropout_cov1")(pl)                                                    # Output: (None, 5, 5, 32)
cov2 = Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), name = "cov2")(drop_out1)          # Output: (None, 3, 3, 32)
pl = MaxPool2D((3, 3), name = "maxp_cov2")(cov2)                                                       # Output: (None, 1, 1, 32)
drop_out2 = Dropout(0.3, name = "dropout_cov2")(pl)                                                    # Output: (None, 1, 1, 32)
flattened = Flatten()(drop_out2)                                                                       # flattening layer Output Shape (None, 32)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_10.csv', index=False)



"""## Score: 0.59076

Accuracy increased in the previous trial so I will change the value of hyperparameters and the change of layers in CNN.

##Trial_11
LSTM Algorithm by using Bidirectional

To prevent overfitting I used regularization l2 in the convolutional layer and dropout layer.

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(LSTM(100, activation='tanh'))(embedded)
bidirect = Dropout(0.3)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (15, 15), name = "cov1", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(in_image)     # 2D Convolutional layer with 32 filters and filter size of 15*15 and regularizer L2. Output: (None, 50, 50, 32)
cov2 = Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), name = "cov2")(cov)       # Output: (None, 48, 48, 32)
pl = MaxPool2D((10, 10), name = "maxp_cov1")(cov2)                                            # Output: (None, 4, 4, 32)
drop_out1 = Dropout(0.3, name = "dropout_cov2")(pl)                                           # Output: (None, 4, 4, 32)
flattened = Flatten()(drop_out1)                                                              # flattening layer Output Shape (None, 512)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_11.csv', index=False)



"""## Score: 0.65353

Accuracy increased in the previous trial so I will change the value of hyperparameters and the change of layers again in CNN.

##Trial_12
LSTM Algorithm without Bidirectional

Image: dropout

### Building a Learning model
"""

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
lstm = LSTM(100, activation='tanh')(embedded)
lstm = Dropout(0.3)(lstm)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (15, 15), name = "cov1", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(in_image)     # 2D Convolutional layer with 32 filters and filter size of 15*15 and regularizer L2. Output: (None, 50, 50, 32)
cov2 = Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), name = "cov2")(cov)    # Output: (None, 48, 48, 32)
pl = MaxPool2D((10, 10), name = "maxp_cov1")(cov2)                                         # Output: (None, 4, 4, 32)
drop_out1 = Dropout(0.3, name = "dropout_cov2")(pl)                                        # Output: (None, 4, 4, 32)
flattened = Flatten()(drop_out1)                                                           # flattening layer Output Shape (None, 288)


# fusion - combinig both
fused = tf.concat([lstm, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_12.csv', index=False)



"""## Score: 0.60788

Accuracy decreased so I will change the value of hyperparameters and add layers in CNN.

##Trial_13
LSTM Algorithm by using Bidirectional

### Building a Learning model
"""

# 2D Convolutional layer with 32 filters and filter size of 16x16 and regularizer L2. Output: (None, 50, 50, 32)
# flattening layer Output Shape (None, 288)

# here we have two inputs. one for image and the other for text.
in_text = keras.Input(batch_shape=(None, max_len))
in_image = keras.Input(batch_shape=(None, 64, 64, 2))

# text part
# simple average of embedding. you can change it to anything else as needed
embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)
bidirect = Bidirectional(LSTM(100, activation='tanh'))(embedded)
bidirect = Dropout(0.3)(bidirect)

# image part 
# simple conv2d. you can change it to anything else as needed
cov = Conv2D(32, (5, 5), name = "cov1", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(in_image) # 2D Convolutional layer with 32 filters and filter size of 5*5 and regularizer L2. Output: (None, 60, 60, 32)
cov1_1= Conv2D(32, (7, 7), name = "cov1_1", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(cov)  # 2D Convolutional layer with 32 filters and filter size of 7*7 and regularizer L2. Output: (None, 54, 54, 32)
pl = MaxPool2D((10, 10), name = "maxp_cov1")(cov1_1)                                                                                # Output: (None, 5, 5, 32)
drop_out1 = Dropout(0.3, name = "dropout_cov1")(pl)                                                                                 # Output: (None, 5, 5, 32)
cov2 = Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01), name = "cov2")(drop_out1) # 2D Convolutional layer with 32 filters and filter size of 3*3 and regularizer L2. Output: (None, 50, 50, 32)
cov2_1 = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01), name = "cov2_1")(cov2)  # 2D Convolutional layer with 32 filters and filter size of 1*1 and regularizer L2. Output: (None, 50, 50, 32)
pl = MaxPool2D((3, 3), name = "maxp_cov2")(cov2_1)                                                                                  # Output: (None, 1, 1, 32)
drop_out2 = Dropout(0.3, name = "dropout_cov2")(pl)                                                                                 # Output: (None, 1, 1, 32)
flattened = Flatten()(drop_out2)                                                                                                    # flattening layer Output Shape (None, 32)


# fusion - combinig both
fused = tf.concat([bidirect, flattened], axis=-1)

# multi-task learning (each is a multi-class classification)
# one dense layer for each task
p_type = Dense(len_type, activation='softmax', name='type')(fused)
p_price = Dense(len_price, activation='softmax', name='price')(fused)


# define model input/output using keys.
model = keras.Model(
    inputs={
        'summary': in_text,
        'image': in_image
    },
    outputs={
        'type': p_type,
        'price': p_price,
    },
)


# compile model with optimizer, loss values for each task, loss 
# weights for each task.
model.compile(
    optimizer=Adam(),
    loss={
        'type': 'sparse_categorical_crossentropy',
        'price': 'sparse_categorical_crossentropy',
    },
    loss_weights={
        'type': 0.5,
        'price': 0.5,       
    },
    metrics={
        'type': ['SparseCategoricalAccuracy'],
        'price': ['SparseCategoricalAccuracy'],
    },
)


model.summary()

"""### Model Training

Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)
"""

history = model.fit(
    x={
        'summary': x_train_text_id,
        'image': x_train_image
    },
    y={
        'type': y_train_type,
        'price': y_train_price,
    },
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5)
    ],
    verbose=1
)

"""### Data Preprocessing (Testing)

Here we use the trained tokenizer to pre-process the testing set.
"""

# loading images:
x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])

# loading summary: (force convert some of the non-string cell to string)
x_test_text = _preprocess(df_test['summary'].astype('str'))

"""### Predition

We can use the model to predict the testing samples.
"""

# we can do prediction on training set
y_predict = model.predict(
    {
        'summary': x_test_text,
        'image': x_test_image
    }
)


# probabilities
price_predicted = y_predict['price']
print(price_predicted)

# categories
price_category_predicted = np.argmax(price_predicted, axis=1)
print(price_category_predicted)

plt.plot(history.history['price_sparse_categorical_accuracy'])
plt.plot(history.history['val_price_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#  (if for kaggle competition and it is about genre prediction)
pd.DataFrame(
    {'id': df_test.id,
     'price': price_category_predicted}
).to_csv('/content/drive/MyDrive/Colab_Notebooks/DM/Project_4/sample_submission_13.csv', index=False)



"""##Score: 0.66168

#**The best solution**

**Trial_13**

Text part: 
* I used embedded and LSTM algorithm(activation function = "tanh") with Bidirectional layer
* Dropout

Image part: 
* CNN(conv, and max-pooling) with different hyperprameters values and layers
* Dropout
* Regularization l2

Score:  0.66168

Private: 0.66902

# Questions

###Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?

- It is not good for sequential data.

- Because the fully connected model does not take into account that dependence between words in the text. 

- Also, the FC NN is not good for image data.
- Because the image needs to extract features from it. We need in the final image one vector. so in FC, we should first use a convolutional layer and then convert the image into flattening and then pass the output into FC layers without using convolution NN will take a lot of time.

### What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?

1- Vanishing Gradients 

* When the derivative will get smaller and smaller and maybe fades away during backpropagation.
* It occurs with the sigmoid(range from 0 to 0.25)and tanh(range from 0 to 1) activation function. Therefore, the updated weight values are small, and then the new weight values are very similar to the old weight values.
* We can avoid this problem using the ReLU activation function because the gradient is 0 for negatives and zero input, and 1 for positive input.

2- Exploding Gradients

*   When the derivative will get larger and larger in backpropagation. that is because large weights. This situation is the exact opposite of the vanishing gradients.

3- GRU/LSTM
* In RNN to train networks, we backpropagate through time, and at each time step or loop operation gradient is being calculated and the gradient is used to update the weights in the networks. Now if the effect of the previous sequence on the layer is small then the relative gradient is calculated small. Then if the gradient of the previous layer is smaller then this makes weights to be assigned to the context smaller and this effect is observed when we deal with longer sequences. Due to this network does not learn the effect of earlier inputs and thus causing the short-term memory problem.

###What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?

1- Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model to get better results. 

**In this assignment I used two feature price and type columns.**

2- Multi-modality learning(MML) aims to build models that can process and relate information from multiple modalities. and it is the most common method in practice is to combine high-level embeddings from the different inputs by concatenating them and then applying a softmax.  

**In this assignment I combine the summary and images when training a model.**

###What is the difference among xgboost, lightgbm and catboost

1- xgboost: 
* Open-source.
* It is available in Python, R, Java, Ruby, Swift, Julia, C, and C++.
* It uses the gradients of different cuts to select the next cut, but XGBoost also uses second derivative, in its ranking of cuts. Computing this next derivative comes at a slight cost, but it also allows a greater estimation of the cut to use.
* It has better learning resources and a more active developer community. 
* High-speed memory.

2- CatBoost:
* It is available in Python, R, C++, Java, and also Rust.
* It is designed for categorical data.
* Symmetric trees.
* Ordered boosting.

3- LightGBM:
* It is available in Python, R, and C.
* LightGBM is almost 7 times faster than xgboost.
* It had the fastest training time as well as the fastest parameter tuning time. 
* Butter with the largest dataset.
* Lower memory usage.
* Parallel learning.
* Better accuracy than another boosting algorithm.

#Refrencess
* https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU

* https://blog.paperspace.com/bidirectional-rnn-keras/

* https://www.numpyninja.com/post/vanishing-and-exploding-gradients-in-neural-networks#:~:text=Exploding%20gradient%20occurs%20when%20the,because%20of%20the%20activation%20function.

* https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/

* https://www.springboard.com/blog/data-science/xgboost-random-forest-catboost-lightgbm/
"""